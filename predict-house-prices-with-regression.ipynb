{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nimport statsmodels.api as smt\n\nfrom sklearn.feature_selection import VarianceThreshold \nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge\nimport xgboost as XGB\nfrom xgboost import XGBRegressor\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-13T17:08:54.818349Z","iopub.execute_input":"2022-05-13T17:08:54.818742Z","iopub.status.idle":"2022-05-13T17:08:57.239304Z","shell.execute_reply.started":"2022-05-13T17:08:54.818652Z","shell.execute_reply":"2022-05-13T17:08:57.238479Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ndf_test = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:08:57.241084Z","iopub.execute_input":"2022-05-13T17:08:57.241594Z","iopub.status.idle":"2022-05-13T17:08:57.322712Z","shell.execute_reply.started":"2022-05-13T17:08:57.241549Z","shell.execute_reply":"2022-05-13T17:08:57.321594Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:08:57.324182Z","iopub.execute_input":"2022-05-13T17:08:57.324417Z","iopub.status.idle":"2022-05-13T17:08:57.361880Z","shell.execute_reply.started":"2022-05-13T17:08:57.324390Z","shell.execute_reply":"2022-05-13T17:08:57.361011Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"print('Shape of train set: {}'.format(df_train.shape))\nprint('Shape of test set: {}'.format(df_test.shape))","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:08:57.364441Z","iopub.execute_input":"2022-05-13T17:08:57.364985Z","iopub.status.idle":"2022-05-13T17:08:57.371907Z","shell.execute_reply.started":"2022-05-13T17:08:57.364936Z","shell.execute_reply":"2022-05-13T17:08:57.370877Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#do a cross check between train and test sets to verify all columns are present in both sets, except for the target variable in train set.\n\nvariables_not_in_train_set = [i for i in df_train.columns if i not in df_test.columns]\nprint('Columns not in test set but present in train set: {}'.format(variables_not_in_train_set))","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:08:57.373575Z","iopub.execute_input":"2022-05-13T17:08:57.374445Z","iopub.status.idle":"2022-05-13T17:08:57.387781Z","shell.execute_reply.started":"2022-05-13T17:08:57.374392Z","shell.execute_reply":"2022-05-13T17:08:57.386157Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df_train.info()","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:08:57.389554Z","iopub.execute_input":"2022-05-13T17:08:57.391800Z","iopub.status.idle":"2022-05-13T17:08:57.441617Z","shell.execute_reply.started":"2022-05-13T17:08:57.391736Z","shell.execute_reply":"2022-05-13T17:08:57.440422Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"We can see quite a number of columns with missing values. We will deal with that later. ","metadata":{}},{"cell_type":"code","source":"#drop the Id column for both train and test sets\ndf_train.drop(columns=['Id'], inplace=True)\n\n# Save the 'Id' list before dropping it from the test set\ntest_Id_list = df_test[\"Id\"].tolist()\ndf_test.drop(columns=['Id'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:08:57.443327Z","iopub.execute_input":"2022-05-13T17:08:57.443647Z","iopub.status.idle":"2022-05-13T17:08:57.453246Z","shell.execute_reply.started":"2022-05-13T17:08:57.443614Z","shell.execute_reply":"2022-05-13T17:08:57.452506Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# 1. Numerical Data ","metadata":{}},{"cell_type":"markdown","source":"We first deal with the data by grouping them into numerical and categorical data. We start off with the numerical data and perform the following investigations:\n\n* Determine the Pearson Correlation coefficient of each features with the target variable. We will drop any features which are deemed poorly correlated with the target variable.\n\n* Drop any features with multicollinearity.\n\n* Drop any features with low variance.\n\n* Determine the distribution of all selected features.\n \n* Impute any missing values or drop any features when imputation doesnt help. \n","metadata":{}},{"cell_type":"code","source":"#Filter out a list containing only numerical variables\ndf_train_numerical = [i for i in df_train.columns if df_train[i].dtype != 'object']\nprint('Numerical variables: ', df_train_numerical)\n\n#compute the correlation heatmap to see which features are highly correlated with target\ncorr_matrix = df_train[df_train_numerical].corr()\nplt.figure(figsize=(20,20), dpi=70)\nsns.heatmap(corr_matrix, cmap=plt.cm.Reds, annot=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:08:57.454362Z","iopub.execute_input":"2022-05-13T17:08:57.455008Z","iopub.status.idle":"2022-05-13T17:09:03.163902Z","shell.execute_reply.started":"2022-05-13T17:08:57.454973Z","shell.execute_reply":"2022-05-13T17:09:03.162538Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"corr_with_SalePrice = df_train[df_train_numerical].corr()['SalePrice'][:-1]\nprint('Features highly correlated with SalePrice: ', corr_with_SalePrice[abs(corr_with_SalePrice) >= 0.5].sort_values(ascending=False).round(2))","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:09:03.165380Z","iopub.execute_input":"2022-05-13T17:09:03.165629Z","iopub.status.idle":"2022-05-13T17:09:03.183319Z","shell.execute_reply.started":"2022-05-13T17:09:03.165602Z","shell.execute_reply":"2022-05-13T17:09:03.182062Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"print('Features fairly correlated with SalePrice: ', corr_with_SalePrice[(abs(corr_with_SalePrice) < 0.5) & (abs(corr_with_SalePrice) > 0.3)].sort_values(ascending=False).round(2))","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:09:03.187138Z","iopub.execute_input":"2022-05-13T17:09:03.187607Z","iopub.status.idle":"2022-05-13T17:09:03.201643Z","shell.execute_reply.started":"2022-05-13T17:09:03.187561Z","shell.execute_reply":"2022-05-13T17:09:03.200833Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"print('Features poorly correlated with SalePrice: ', corr_with_SalePrice[abs(corr_with_SalePrice) < 0.3].sort_values().round(2))","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:09:03.203086Z","iopub.execute_input":"2022-05-13T17:09:03.203557Z","iopub.status.idle":"2022-05-13T17:09:03.219740Z","shell.execute_reply.started":"2022-05-13T17:09:03.203514Z","shell.execute_reply":"2022-05-13T17:09:03.218914Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Features that have a pearson correlation coefficient of less than 0.30 will be dropped, as they have very little to no linear relationship with our target variable. ","metadata":{}},{"cell_type":"code","source":"#strong features with corr more than 0.5\nstrong_features = corr_with_SalePrice[corr_with_SalePrice >= 0.5].sort_values(ascending=False).index\nstrong_list = [x for x in strong_features]\nstrong_list.append('SalePrice')\n\ndef reg_plot(df, features, rows, col):\n    fig = plt.figure(figsize=(19,19), dpi=70)\n    for i, feature in enumerate(features):\n        if feature != 'SalePrice':\n            ax = fig.add_subplot(rows, col, i+1)\n            sns.regplot(x=feature, y='SalePrice', data=df, line_kws={'color':'black'})\n            ax.set_xlabel(feature)\n            ax.set_ylabel('SalePrice')\n    \nreg_plot(df_train[strong_list], strong_list, 4, 3)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:09:03.221122Z","iopub.execute_input":"2022-05-13T17:09:03.221805Z","iopub.status.idle":"2022-05-13T17:09:06.744032Z","shell.execute_reply.started":"2022-05-13T17:09:03.221766Z","shell.execute_reply":"2022-05-13T17:09:06.742944Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#features with fair corr between 0.3 to 0.5\nfair_features = corr_with_SalePrice[(corr_with_SalePrice < 0.5) & (corr_with_SalePrice > 0.3)].sort_values(ascending=False).index\nfair_list = [x for x in fair_features]\nfair_list.append('SalePrice')\n\nreg_plot(df_train[fair_list], fair_list, 4, 3)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:09:06.745434Z","iopub.execute_input":"2022-05-13T17:09:06.745687Z","iopub.status.idle":"2022-05-13T17:09:09.567760Z","shell.execute_reply.started":"2022-05-13T17:09:06.745658Z","shell.execute_reply":"2022-05-13T17:09:09.567126Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#detecting multicollinearity amongst our numerical variables\n\nrows, cols = df_train[df_train_numerical].shape\ncolumns_list = list(df_train[df_train_numerical].columns)\n\ndf = []\ncorr = df_train[df_train_numerical].corr().values\nfor i in range(cols):\n    for j in range(i+1, cols):\n        if corr[i,j] > 0.7:\n            df.append([columns_list[i], columns_list[j], corr[i,j].round(2)])\n\ndf1 = pd.DataFrame(df, columns=['Variable 1', 'Variable 2', 'Corr']).sort_values(by=['Corr'], ascending=False)\ndf1[df1['Variable 2'] != 'SalePrice']","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:09:09.568834Z","iopub.execute_input":"2022-05-13T17:09:09.569990Z","iopub.status.idle":"2022-05-13T17:09:09.602466Z","shell.execute_reply.started":"2022-05-13T17:09:09.569923Z","shell.execute_reply":"2022-05-13T17:09:09.601783Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"From our correlation heatmap, we detected 4 pairs of features which are multicollinear, with correlation values of more than 0.80. As such, we will be dropping each of the features within each pairs.","metadata":{}},{"cell_type":"code","source":"#keeping variables with fair to strong corr with SalePrice\nstrong_corr = corr_with_SalePrice[abs(corr_with_SalePrice) >= 0.5].sort_values(ascending=False).round(2)\nfair_corr = corr_with_SalePrice[(abs(corr_with_SalePrice) < 0.5) & (abs(corr_with_SalePrice) > 0.3)].sort_values(ascending=False).round(2)\n\nfeatures_to_keep = list(strong_corr.index) + list(fair_corr.index)\n\n#drop variables with multicollinearity\nnumerical_to_drop = ['GarageCars', 'GarageYrBlt', 'TotRmsAbvGrd', '1stFlrSF']\nnumerical_to_keep = [x for x in features_to_keep if x not in numerical_to_drop]\nnumerical_to_keep.append('SalePrice')\n\nprint('Numerical features to be kept:')\nprint(numerical_to_keep)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:09:09.603536Z","iopub.execute_input":"2022-05-13T17:09:09.604582Z","iopub.status.idle":"2022-05-13T17:09:09.617215Z","shell.execute_reply.started":"2022-05-13T17:09:09.604508Z","shell.execute_reply":"2022-05-13T17:09:09.616184Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"#updated train set after dropping\ndf_train1_numerical = df_train[numerical_to_keep]\n\n#updated test set after dropping\nnumerical_to_keep.remove('SalePrice')\ndf_test1_numerical = df_test[numerical_to_keep]\n\n#check to ensure columns are dropped\nprint(df_train1_numerical.shape)\nprint(df_test1_numerical.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:09:09.618765Z","iopub.execute_input":"2022-05-13T17:09:09.619521Z","iopub.status.idle":"2022-05-13T17:09:09.638564Z","shell.execute_reply.started":"2022-05-13T17:09:09.619470Z","shell.execute_reply":"2022-05-13T17:09:09.637186Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#cross check to all columns are present in both dataset\nvariables_not_in_train_set = [i for i in df_train1_numerical.columns if i not in df_test1_numerical.columns]\nprint('Columns not in test set but present in train set: ', variables_not_in_train_set)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:09:09.639882Z","iopub.execute_input":"2022-05-13T17:09:09.640989Z","iopub.status.idle":"2022-05-13T17:09:09.653035Z","shell.execute_reply.started":"2022-05-13T17:09:09.640918Z","shell.execute_reply":"2022-05-13T17:09:09.652106Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"After dropping features, we double check again to ensure the number of columns and rows for both train and test sets are tallied. ","metadata":{}},{"cell_type":"markdown","source":"Next, we check for any features with small variations (low variance). These features are likely to have small impact on the final price of the house. Keeping these features is of no use to our prediction model. As such, we will drop these features, if any. We set the threshold at 5%, meaning that any features with 95% or more similar values will be dropped.","metadata":{}},{"cell_type":"code","source":"#removing quasi-features that have low variance\n\nvar = VarianceThreshold(threshold = 0.05) \nvar.fit(df_train1_numerical)\n\ncol_to_drop = [x for x in df_train1_numerical.columns if x not in df_train1_numerical.columns[var.get_support()]]\nprint('Columns with low variance to be dropped: ', col_to_drop)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:09:09.654237Z","iopub.execute_input":"2022-05-13T17:09:09.654688Z","iopub.status.idle":"2022-05-13T17:09:09.670014Z","shell.execute_reply.started":"2022-05-13T17:09:09.654654Z","shell.execute_reply":"2022-05-13T17:09:09.669071Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"#have a look at the overall distributions of our remaining numerical variables\n\ndf_train1_numerical.hist(bins=50, figsize=(15,15))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:09:09.671282Z","iopub.execute_input":"2022-05-13T17:09:09.671543Z","iopub.status.idle":"2022-05-13T17:09:12.778633Z","shell.execute_reply.started":"2022-05-13T17:09:09.671505Z","shell.execute_reply":"2022-05-13T17:09:12.777510Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"From the distributions, we actually notice a mixture of both discrete and continuous features. ","metadata":{}},{"cell_type":"markdown","source":"# 1.1 Missing Values","metadata":{}},{"cell_type":"markdown","source":"We first with missing values from the train set.\n","metadata":{}},{"cell_type":"code","source":"#filter out columns with missing values and calculate its proportion compared to total number of data\n\ncol_missing_values_train = [x for x in df_train1_numerical.columns if df_train1_numerical[x].isnull().any()]\ncol_df_missing_train = pd.DataFrame(df_train1_numerical[col_missing_values_train].isnull().sum(), columns=['No. of Missing Values'])\ncol_df_missing_train['Percent of total data'] = (100 * col_df_missing_train['No. of Missing Values'] / df_train1_numerical.shape[0]).round(2)\n\nprint('Numerical columns with missing values: ', col_missing_values_train)\nplt.figure(figsize=(8,8), dpi=70)\nsns.set_style('darkgrid')\nsns.barplot(x=col_df_missing_train.index, y=col_df_missing_train['Percent of total data'], data=col_df_missing_train)\nplt.title('% of missing values')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:09:12.780068Z","iopub.execute_input":"2022-05-13T17:09:12.780297Z","iopub.status.idle":"2022-05-13T17:09:12.993793Z","shell.execute_reply.started":"2022-05-13T17:09:12.780269Z","shell.execute_reply":"2022-05-13T17:09:12.993128Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"The missing values in 'LotFrontage' makes up more than 17.5% of the total number of rows. \n\nWe will next proceed to impute missing values for both features with their respective median values. ","metadata":{}},{"cell_type":"code","source":"simple_imp = SimpleImputer(strategy = 'median')\n\nimputed_df_numerical_train = pd.DataFrame(simple_imp.fit_transform(df_train1_numerical))\nimputed_df_numerical_train.columns = df_train1_numerical.columns","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:09:12.995631Z","iopub.execute_input":"2022-05-13T17:09:12.996209Z","iopub.status.idle":"2022-05-13T17:09:13.010622Z","shell.execute_reply.started":"2022-05-13T17:09:12.996160Z","shell.execute_reply":"2022-05-13T17:09:13.009683Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"sns.set(rc={'figure.figsize':(14,12)})\nfig, ax = plt.subplots(2, 2)\n\nfor pos, feature in enumerate(col_missing_values_train):\n    sns.histplot(df_train1_numerical[feature], bins=30, kde=True, ax = ax[pos, 0])\n    ax[pos, 0].set_ylabel('Before Imputation')\n    \n    sns.histplot(imputed_df_numerical_train[feature], bins=30, kde=True, ax = ax[pos, 1])\n    ax[pos, 1].set_ylabel('After Imputation')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:09:13.011987Z","iopub.execute_input":"2022-05-13T17:09:13.012251Z","iopub.status.idle":"2022-05-13T17:09:14.042493Z","shell.execute_reply.started":"2022-05-13T17:09:13.012217Z","shell.execute_reply":"2022-05-13T17:09:14.041249Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"We check the distribution of the two features to see if there is any major change or shift to the distributions. 'LotFrontage' shows a significant change in distribution (as shown by the shift in its mode). As such, we will drop the feature 'LotFrontage' and keep 'MasVnrArea'.","metadata":{}},{"cell_type":"code","source":"#dropping the feature 'LotFrontage' from our train set\n\nimputed_df_numerical_train.drop(['LotFrontage'], axis=1, inplace=True)\n\nimputed_df_numerical_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:09:14.043867Z","iopub.execute_input":"2022-05-13T17:09:14.044105Z","iopub.status.idle":"2022-05-13T17:09:14.073002Z","shell.execute_reply.started":"2022-05-13T17:09:14.044074Z","shell.execute_reply":"2022-05-13T17:09:14.071389Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"To account equally for both train and test sets, we will also drop the feature 'LotFrontage' from our test set.","metadata":{}},{"cell_type":"code","source":"#dropping the feature 'LotFrontage' from our test set\n\ndf_test1_numerical.drop(['LotFrontage'], axis=1, inplace=True)\n\ndf_test1_numerical.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:09:14.074580Z","iopub.execute_input":"2022-05-13T17:09:14.074802Z","iopub.status.idle":"2022-05-13T17:09:14.092912Z","shell.execute_reply.started":"2022-05-13T17:09:14.074776Z","shell.execute_reply":"2022-05-13T17:09:14.092327Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"Next, we deal with missing values from the test set.","metadata":{}},{"cell_type":"code","source":"col_missing_values_test = [x for x in df_test1_numerical.columns if df_test1_numerical[x].isnull().any()]\ncol_df_missing_test = pd.DataFrame(df_test1_numerical[col_missing_values_test].isnull().sum(), columns=['No. of Missing Values'])\ncol_df_missing_test['Percent of total data'] = (100 * col_df_missing_test['No. of Missing Values'] / df_test1_numerical.shape[0]).round(2)\n\nprint('Numerical columns with missing values: ', col_missing_values_test)\nplt.figure(figsize=(8,8), dpi=70)\nsns.set_style('darkgrid')\nsns.barplot(x=col_df_missing_test.index, y=col_df_missing_test['Percent of total data'], data=col_df_missing_test)\nplt.title('% of missing values')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:09:14.093892Z","iopub.execute_input":"2022-05-13T17:09:14.094865Z","iopub.status.idle":"2022-05-13T17:09:14.580873Z","shell.execute_reply.started":"2022-05-13T17:09:14.094828Z","shell.execute_reply":"2022-05-13T17:09:14.579059Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"The missing values in 'MasVnrArea' makes up more than 1% of the total number of rows, while the rest make up less than 0.2% of the total. \n\nWe will next proceed to impute missing values for all features with their respective median values.","metadata":{}},{"cell_type":"code","source":"#impute the missing columns with median values\n\nsimple_imp = SimpleImputer(strategy = 'median')\n\nimputed_df_numerical_test = pd.DataFrame(simple_imp.fit_transform(df_test1_numerical))\nimputed_df_numerical_test.columns = df_test1_numerical.columns","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:09:14.582268Z","iopub.execute_input":"2022-05-13T17:09:14.582703Z","iopub.status.idle":"2022-05-13T17:09:14.595083Z","shell.execute_reply.started":"2022-05-13T17:09:14.582668Z","shell.execute_reply":"2022-05-13T17:09:14.594130Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"sns.set(rc={'figure.figsize':(17,15)})\nfig, ax = plt.subplots(4, 2)\n\nfor pos, feature in enumerate(col_missing_values_test):\n    sns.histplot(df_test1_numerical[feature], bins=30, kde=True, ax=ax[pos, 0])\n    ax[pos, 0].set_ylabel('Before Imputation')\n    \n    sns.histplot(imputed_df_numerical_test[feature], bins=30, kde=True, ax=ax[pos, 1])\n    ax[pos, 1].set_ylabel('After Imputation')\n    \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:09:14.597811Z","iopub.execute_input":"2022-05-13T17:09:14.598492Z","iopub.status.idle":"2022-05-13T17:09:16.601202Z","shell.execute_reply.started":"2022-05-13T17:09:14.598436Z","shell.execute_reply":"2022-05-13T17:09:16.600328Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"There are no significant changes to the distribution of our features above after imputation. Hence, we shall keep all of them.","metadata":{}},{"cell_type":"code","source":"#check to ensure all columns have no missing values for both data sets\n\npd.DataFrame([imputed_df_numerical_train.isnull().sum(), imputed_df_numerical_test.isnull().sum()], index=['Train', 'Test'])","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:09:16.606680Z","iopub.execute_input":"2022-05-13T17:09:16.606962Z","iopub.status.idle":"2022-05-13T17:09:16.632831Z","shell.execute_reply.started":"2022-05-13T17:09:16.606930Z","shell.execute_reply":"2022-05-13T17:09:16.631940Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"Double check both data sets again to ensure no more missing values in all columns","metadata":{}},{"cell_type":"code","source":"print('Final number of numerical variables in train set: ', imputed_df_numerical_train.shape)\nprint('Final number of numerical variables in test set: ', imputed_df_numerical_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:09:16.634179Z","iopub.execute_input":"2022-05-13T17:09:16.634524Z","iopub.status.idle":"2022-05-13T17:09:16.641886Z","shell.execute_reply.started":"2022-05-13T17:09:16.634480Z","shell.execute_reply":"2022-05-13T17:09:16.640770Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"#cross check to all columns are present in both dataset\n\nvariables_not_in_train_set = [i for i in imputed_df_numerical_train.columns if i not in imputed_df_numerical_test.columns]\nprint('Columns not in test set but present in train set: ', variables_not_in_train_set)\n\nvariables_not_in_test_set = [i for i in imputed_df_numerical_test.columns if i not in imputed_df_numerical_train.columns]\nprint('Columns not in test set but present in train set: ', variables_not_in_test_set)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:09:16.643166Z","iopub.execute_input":"2022-05-13T17:09:16.643393Z","iopub.status.idle":"2022-05-13T17:09:16.657533Z","shell.execute_reply.started":"2022-05-13T17:09:16.643365Z","shell.execute_reply":"2022-05-13T17:09:16.656756Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"A cross check of both our train and test sets again shows all columns are the same and properly accounted for. ","metadata":{}},{"cell_type":"markdown","source":"# 2. Categorical data","metadata":{}},{"cell_type":"markdown","source":"Next up, we will deal with the categorical data by performing the following investigations:\n\n* Determine the distribution of all categorical features with bar charts.\n\n* Drop any features with low variance.\n \n* Impute any missing values or drop any features when imputation doesnt help. \n\n* Determine the relationship between the categorical features and our target variable with box plots.","metadata":{}},{"cell_type":"code","source":"df_categorical = [i for i in df_train.columns if df_train[i].dtype == 'object']\n\nprint(f'Categorical variables: {df_categorical}\\n')\nprint(f'Number of categorical variables: {len(df_categorical)}\\n')","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:09:16.658702Z","iopub.execute_input":"2022-05-13T17:09:16.659057Z","iopub.status.idle":"2022-05-13T17:09:16.675004Z","shell.execute_reply.started":"2022-05-13T17:09:16.659019Z","shell.execute_reply":"2022-05-13T17:09:16.673944Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"#define our train and test df, only include categorical variables\n\ndf_train_categorical = pd.concat([df_train[df_categorical], df_train['SalePrice']], axis=1)\ndf_test_categorical = df_test[df_categorical]","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:09:16.676691Z","iopub.execute_input":"2022-05-13T17:09:16.677639Z","iopub.status.idle":"2022-05-13T17:09:16.690970Z","shell.execute_reply.started":"2022-05-13T17:09:16.677562Z","shell.execute_reply":"2022-05-13T17:09:16.689397Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"Next, we find out the distribution of our variables.","metadata":{}},{"cell_type":"code","source":"#overview of distributions with bar charts\n\nfig, ax = plt.subplots(15, 3, figsize=(15, 40))\n\nfor i, ax in enumerate(fig.axes):\n    if i < len(df_categorical):\n        sns.countplot(x=df_categorical[i], data=df_train[df_categorical], ax=ax)\n        ax.set_xticklabels(ax.xaxis.get_majorticklabels(), rotation=90)\n        ax.set_xlabel(df_categorical[i])\n\nfig.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:09:16.692425Z","iopub.execute_input":"2022-05-13T17:09:16.693049Z","iopub.status.idle":"2022-05-13T17:09:23.658393Z","shell.execute_reply.started":"2022-05-13T17:09:16.693016Z","shell.execute_reply":"2022-05-13T17:09:23.657331Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"Through eyeballing, we can detect a few variables that are dominated by a single group. We can remove these features as they have little to no impact on the prediction of house prices. ","metadata":{}},{"cell_type":"code","source":"catcol_to_be_dropped = ['Street', \n                        'LandContour', \n                        'Utilities', \n                        'LandSlope', \n                        'Condition2', \n                        'RoofMatl', \n                        'BsmtCond', \n                        'BsmtFinType2',\n                        'Heating', \n                        'CentralAir', \n                        'Electrical', \n                        'Functional', \n                        'GarageQual', \n                        'GarageCond',\n                        'PavedDrive']","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:09:23.659670Z","iopub.execute_input":"2022-05-13T17:09:23.659893Z","iopub.status.idle":"2022-05-13T17:09:23.665865Z","shell.execute_reply.started":"2022-05-13T17:09:23.659865Z","shell.execute_reply":"2022-05-13T17:09:23.664719Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"#drop the columns for both train and test sets\n\ndf_train_categorical.drop(catcol_to_be_dropped, axis=1, inplace=True)\ndf_test_categorical.drop(catcol_to_be_dropped, axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:09:23.667188Z","iopub.execute_input":"2022-05-13T17:09:23.667479Z","iopub.status.idle":"2022-05-13T17:09:23.681650Z","shell.execute_reply.started":"2022-05-13T17:09:23.667423Z","shell.execute_reply":"2022-05-13T17:09:23.680622Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"#check to see if columns are dropped\n\nprint(f'Train set: {df_train_categorical.shape}\\n')\nprint(f'Test set: {df_test_categorical.shape}\\n')","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:09:23.683508Z","iopub.execute_input":"2022-05-13T17:09:23.683836Z","iopub.status.idle":"2022-05-13T17:09:23.698049Z","shell.execute_reply.started":"2022-05-13T17:09:23.683789Z","shell.execute_reply":"2022-05-13T17:09:23.696890Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"We will next visualize how the features vary with our target variable with box plots.","metadata":{}},{"cell_type":"code","source":"#overview of features vs target variable\n\nfig, ax = plt.subplots(10, 3, figsize=(15, 40))\n\nfor i, ax in enumerate(fig.axes):\n    if i < len(df_train_categorical.columns)-1:\n        sns.boxplot(x=df_train_categorical.columns[i], y='SalePrice', data=df_train_categorical, ax=ax)\n        ax.set_xticklabels(ax.xaxis.get_majorticklabels(), rotation = 90)\n        \nfig.tight_layout()\nplt.show()\n        ","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:09:23.699678Z","iopub.execute_input":"2022-05-13T17:09:23.699928Z","iopub.status.idle":"2022-05-13T17:09:31.374796Z","shell.execute_reply.started":"2022-05-13T17:09:23.699898Z","shell.execute_reply":"2022-05-13T17:09:31.374166Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"From the boxplots, we notice some variables have similar pattern of group distributions, which might suggest co-dependency between each pairs. The following pairs of variables have very similar distribution:\n\n* 'ExterQual' and 'BsmtQual' (pair_1)\n* 'MasVnrType' and 'ExterQual' (pair_2)\n* 'Exterior1st' and 'Exterior2nd' (pair_3)\n* 'GarageType' and 'SaleCondition' (pair_4)\n* 'MasVnrType' and 'BsmtQual' (pair_5)\n* 'BsmtQual' and 'KitchenQual' (pair_6)\n\nwe shall perform a Chi-squared test of independence for each pair of variables at 5% significance level to determine whether or not they have any strong dependency. For any pair with strong dependency, we will drop one of variable within the pair.  ","metadata":{}},{"cell_type":"code","source":"#loop over each pair and crosstab all of them\n\nv1 = ['ExterQual', 'MasVnrType', 'Exterior1st', 'GarageType', 'MasVnrType', 'BsmtQual']\nv2 = ['BsmtQual', 'ExterQual', 'Exterior2nd', 'SaleCondition', 'BsmtQual', 'KitchenQual']\n\ncrosstab_arrays = []\nfor i, j in zip(v1, v2):\n    x = np.array(pd.crosstab(df_train_categorical[i], df_train_categorical[j]))\n    crosstab_arrays.append(x)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:09:31.376028Z","iopub.execute_input":"2022-05-13T17:09:31.376420Z","iopub.status.idle":"2022-05-13T17:09:31.448098Z","shell.execute_reply.started":"2022-05-13T17:09:31.376389Z","shell.execute_reply":"2022-05-13T17:09:31.447526Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"To simplify the crosstabbing process, we loop over the 6 pairs and assign their crosstabs to a list. Subsequently, we will proceed to conduct our Chi_squared test for each pair separately. ","metadata":{}},{"cell_type":"markdown","source":"We set the significance level for our test to be at 5%. We set the hypotheses as follow:\n\n**Null hypothesis (H0)**: Variables are independent\n\n**Alternative hypothesis (H1)**: Variables are dependent","metadata":{}},{"cell_type":"code","source":"#evaluate p-values of chi-squared test\n\nfor i, j in enumerate(zip(v1, v2)):\n    chi2sq_result = stats.chi2_contingency(crosstab_arrays[i])\n    print(f\"p-value of Chi-squared test bewteen {j[0]} and {j[1]} is: {chi2sq_result[1]}\")\n    if chi2sq_result[1] < 0.05:\n        print(f'Variables are dependent (Reject H0)\\n')\n    else:\n        print(f'Variables are independent (Fail to reject H0)\\n')","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:09:31.449335Z","iopub.execute_input":"2022-05-13T17:09:31.449703Z","iopub.status.idle":"2022-05-13T17:09:31.460702Z","shell.execute_reply.started":"2022-05-13T17:09:31.449674Z","shell.execute_reply":"2022-05-13T17:09:31.459758Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"For all tests, we reject H0 and conclude that the variables are all dependent. As such, we have to drop one of the co-dependent variables for all 6 pairs.","metadata":{}},{"cell_type":"code","source":"df_train_categorical.drop(v1, axis=1, inplace=True)\ndf_test_categorical.drop(v1, axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:09:31.462158Z","iopub.execute_input":"2022-05-13T17:09:31.462436Z","iopub.status.idle":"2022-05-13T17:09:31.472710Z","shell.execute_reply.started":"2022-05-13T17:09:31.462405Z","shell.execute_reply":"2022-05-13T17:09:31.471894Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"#check to see if columns are dropped\n\nprint(f'Train set: {df_train_categorical.shape}\\n')\nprint(f'Test set: {df_test_categorical.shape}\\n')","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:09:31.473919Z","iopub.execute_input":"2022-05-13T17:09:31.474254Z","iopub.status.idle":"2022-05-13T17:09:31.486776Z","shell.execute_reply.started":"2022-05-13T17:09:31.474226Z","shell.execute_reply":"2022-05-13T17:09:31.485822Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"# 2.1 Missing Values","metadata":{}},{"cell_type":"markdown","source":"We will now investigate all categorical columns with missing values and impute, or drop them if necessary.","metadata":{}},{"cell_type":"code","source":"#visualize train set columns with missing values\n\ndf_missing_categorical_train = pd.DataFrame(100 * df_train_categorical.isnull().sum() / df_train_categorical.shape[0], columns=['Percent of total data']).sort_values(by='Percent of total data', ascending=False)\n\ndf_filtered_missing_train = df_missing_categorical_train[df_missing_categorical_train['Percent of total data'] > 0]\n\nsns.barplot(y=df_filtered_missing_train.index, x='Percent of total data', orient='h', data=df_filtered_missing_train)\nplt.title('Missing data for Categorical Variables (Train set)', fontsize=18)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:09:31.488522Z","iopub.execute_input":"2022-05-13T17:09:31.488924Z","iopub.status.idle":"2022-05-13T17:09:31.794200Z","shell.execute_reply.started":"2022-05-13T17:09:31.488892Z","shell.execute_reply":"2022-05-13T17:09:31.793131Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"We shall drop any variable with missing values comprising more than 30% of its total data.  ","metadata":{}},{"cell_type":"code","source":"NaN_col_to_drop_train = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu']\n\n#drop the variables in both train and test set\n\ndf_train_categorical.drop(NaN_col_to_drop_train, axis=1, inplace=True)\ndf_test_categorical.drop(NaN_col_to_drop_train, axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:09:31.795672Z","iopub.execute_input":"2022-05-13T17:09:31.795902Z","iopub.status.idle":"2022-05-13T17:09:31.803332Z","shell.execute_reply.started":"2022-05-13T17:09:31.795865Z","shell.execute_reply":"2022-05-13T17:09:31.802535Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"For the remaining variables with less than 5% of missing values, we shall impute all missing values with values from their corresponding modal classes. ","metadata":{}},{"cell_type":"code","source":"#impute our missing values in train set with mode\n\nNaN_col_to_impute = {'GarageFinish': df_train_categorical['GarageFinish'].mode().iloc[0],\n 'BsmtExposure': df_train_categorical['BsmtExposure'].mode().iloc[0],\n 'BsmtFinType1': df_train_categorical['BsmtFinType1'].mode().iloc[0]}\n\nimputed_df_train_categorical = df_train_categorical.fillna(value=NaN_col_to_impute)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:09:31.804909Z","iopub.execute_input":"2022-05-13T17:09:31.805938Z","iopub.status.idle":"2022-05-13T17:09:31.821377Z","shell.execute_reply.started":"2022-05-13T17:09:31.805904Z","shell.execute_reply":"2022-05-13T17:09:31.819829Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"Similarly, we also check for variables with missing values in our test set, and then impute or drop them if necessary.","metadata":{"execution":{"iopub.status.busy":"2022-05-11T11:57:04.897682Z","iopub.execute_input":"2022-05-11T11:57:04.898331Z","iopub.status.idle":"2022-05-11T11:57:04.923022Z","shell.execute_reply.started":"2022-05-11T11:57:04.898289Z","shell.execute_reply":"2022-05-11T11:57:04.922309Z"}}},{"cell_type":"code","source":"#visualize test set columns with missing values\n\ndf_missing_categorical_test = pd.DataFrame(100 * df_test_categorical.isnull().sum() / df_test_categorical.shape[0], columns=['Percent of total data']).sort_values(by='Percent of total data', ascending=False)\n\ndf_filtered_missing_test = df_missing_categorical_test[df_missing_categorical_test['Percent of total data'] > 0]\n\nsns.barplot(y=df_filtered_missing_test.index, x='Percent of total data', orient='h', data=df_filtered_missing_test)\nplt.title('Missing data for Categorical Variables (Test set)', fontsize=18)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:09:31.822839Z","iopub.execute_input":"2022-05-13T17:09:31.823102Z","iopub.status.idle":"2022-05-13T17:09:32.122134Z","shell.execute_reply.started":"2022-05-13T17:09:31.823071Z","shell.execute_reply":"2022-05-13T17:09:32.121339Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"Most of the missing values for our test set columns are less than 5%. As such, we will impute all missing values with their corresponding modal classes. ","metadata":{}},{"cell_type":"code","source":"#impute our missing values in test set with mode\n\nNaN_col_to_impute = {'GarageFinish': df_test_categorical['GarageFinish'].mode().iloc[0],\n                     'BsmtExposure': df_test_categorical['BsmtExposure'].mode().iloc[0],\n                     'BsmtFinType1': df_test_categorical['BsmtFinType1'].mode().iloc[0],\n                     'MSZoning': df_test_categorical['MSZoning'].mode().iloc[0],\n                     'SaleType': df_test_categorical['SaleType'].mode().iloc[0],\n                     'KitchenQual': df_test_categorical['KitchenQual'].mode().iloc[0],\n                     'Exterior2nd': df_test_categorical['Exterior2nd'].mode().iloc[0]}\n\nimputed_df_test_categorical = df_test_categorical.fillna(value=NaN_col_to_impute)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:09:32.123507Z","iopub.execute_input":"2022-05-13T17:09:32.124625Z","iopub.status.idle":"2022-05-13T17:09:32.141085Z","shell.execute_reply.started":"2022-05-13T17:09:32.124579Z","shell.execute_reply":"2022-05-13T17:09:32.139939Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"#finally, check to ensure there are no more categorical columns with missing values in train and test set\n\npd.DataFrame([imputed_df_train_categorical.isnull().sum(), imputed_df_test_categorical.isnull().sum()], index=['Train', 'Test'])","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:09:32.142261Z","iopub.execute_input":"2022-05-13T17:09:32.142535Z","iopub.status.idle":"2022-05-13T17:09:32.187302Z","shell.execute_reply.started":"2022-05-13T17:09:32.142504Z","shell.execute_reply":"2022-05-13T17:09:32.186042Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"#final check to see if columns are dropped and tallied between train and test set\n\nprint(f'Train set: {df_train_categorical.shape}\\n')\nprint(f'Test set: {df_test_categorical.shape}\\n')","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:09:32.188897Z","iopub.execute_input":"2022-05-13T17:09:32.189827Z","iopub.status.idle":"2022-05-13T17:09:32.196696Z","shell.execute_reply.started":"2022-05-13T17:09:32.189780Z","shell.execute_reply":"2022-05-13T17:09:32.195691Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"**Encoding Categorical Variables**","metadata":{}},{"cell_type":"markdown","source":"Before we can use the data for training, we have to transform them into numerical values. This will be done by using one-hot encoding.","metadata":{}},{"cell_type":"code","source":"#transform our train set with pandas function get_dummies()\n\ndf_train_categorical.drop(['SalePrice'], axis=1, inplace=True)\n\ndf_train_categorical_dummies = pd.get_dummies(df_train_categorical, drop_first=True)\ndf_train_categorical_dummies.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:09:32.200200Z","iopub.execute_input":"2022-05-13T17:09:32.200502Z","iopub.status.idle":"2022-05-13T17:09:32.245577Z","shell.execute_reply.started":"2022-05-13T17:09:32.200441Z","shell.execute_reply":"2022-05-13T17:09:32.244699Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"#transform our test set with pandas function get_dummies()\n\ndf_test_categorical_dummies = pd.get_dummies(df_test_categorical, drop_first=True)\ndf_test_categorical_dummies.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:09:32.247013Z","iopub.execute_input":"2022-05-13T17:09:32.248203Z","iopub.status.idle":"2022-05-13T17:09:32.287216Z","shell.execute_reply.started":"2022-05-13T17:09:32.248132Z","shell.execute_reply":"2022-05-13T17:09:32.286128Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"#check to see which variables are not present in either sets after encoding\n\nvariables_not_in_test_set = [i for i in df_train_categorical_dummies.columns if i not in df_test_categorical_dummies.columns]\nvariables_not_in_train_set = [i for i in df_test_categorical_dummies.columns if i not in df_train_categorical_dummies.columns]\nprint('Columns not in train set but present in test set: ', variables_not_in_test_set)\nprint('Columns not in test set but present in train set: ', variables_not_in_train_set)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:09:32.288483Z","iopub.execute_input":"2022-05-13T17:09:32.288709Z","iopub.status.idle":"2022-05-13T17:09:32.297996Z","shell.execute_reply.started":"2022-05-13T17:09:32.288670Z","shell.execute_reply":"2022-05-13T17:09:32.296906Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"'HouseStyle_2.5Fin', 'Exterior2nd_Other' are the two dummy variables not present in the test set. Hence, we will drop these two columns in our train set.","metadata":{}},{"cell_type":"code","source":"#dropping the two dummy variables and check again\n\ndf_train_categorical_dummies.drop(['HouseStyle_2.5Fin', 'Exterior2nd_Other'], axis=1, inplace=True)\n\nvariables_not_in_test_set = [i for i in df_train_categorical_dummies.columns if i not in df_test_categorical_dummies.columns]\nprint('Columns not in test set but present in train set: ', variables_not_in_train_set)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:09:32.299269Z","iopub.execute_input":"2022-05-13T17:09:32.300016Z","iopub.status.idle":"2022-05-13T17:09:32.317266Z","shell.execute_reply.started":"2022-05-13T17:09:32.299975Z","shell.execute_reply":"2022-05-13T17:09:32.316641Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"print(f'Train set: {df_train_categorical_dummies.shape}\\n')\nprint(f'Test set: {df_test_categorical_dummies.shape}\\n')","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:09:32.318624Z","iopub.execute_input":"2022-05-13T17:09:32.319037Z","iopub.status.idle":"2022-05-13T17:09:32.331070Z","shell.execute_reply.started":"2022-05-13T17:09:32.318989Z","shell.execute_reply":"2022-05-13T17:09:32.330100Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"**Merge both Numerical and Categorical data together as final data**","metadata":{}},{"cell_type":"code","source":"new_train_set = pd.concat([imputed_df_numerical_train, df_train_categorical_dummies], axis=1)\nprint(f'Shape of new train set: {new_train_set.shape}\\n')\n\nnew_test_set = pd.concat([imputed_df_numerical_test, df_test_categorical_dummies], axis=1)\nprint(f'Shape of new test set: {new_test_set.shape}\\n')","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:09:32.332520Z","iopub.execute_input":"2022-05-13T17:09:32.332756Z","iopub.status.idle":"2022-05-13T17:09:32.347043Z","shell.execute_reply.started":"2022-05-13T17:09:32.332726Z","shell.execute_reply":"2022-05-13T17:09:32.345831Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":"# 3. Feature Transformation","metadata":{}},{"cell_type":"markdown","source":"Right at the start of our analysis, some numerical features were spotted to be skewed. To ensure a good accuracy for our model, we need to transform these features such that their distributions are closer to a normal distribution. Ideally, a normally distributed data allows the machine learning model to learn from a wider range of data, rather than learning data that are skewed and ending up with a bias model. \n\nBefore we proceed to apply data transformations, we need to convert 'YearBuilt' and 'YearRemodAdd' to age of the house since it was first built, and age of the house since it was first remodelled. It does not make sense to transform 'YearBuilt' and 'YearRemodAdd' as Year is an ordinal feature. As such, they will both be removed after conversion to age.  ","metadata":{}},{"cell_type":"code","source":"#converting to age by creating new column 'Age Since Built'\n\nnew_train_set['Age Since Built'] = new_train_set['YearBuilt'].max() - new_train_set['YearBuilt']\nnew_test_set['Age Since Built'] = new_test_set['YearBuilt'].max() - new_test_set['YearBuilt']\n\n#drop 'YearBuilt'\n\nnew_train_set.drop([\"YearBuilt\"], axis=1, inplace=True)\nnew_test_set.drop([\"YearBuilt\"], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:09:32.348361Z","iopub.execute_input":"2022-05-13T17:09:32.349049Z","iopub.status.idle":"2022-05-13T17:09:32.364111Z","shell.execute_reply.started":"2022-05-13T17:09:32.348995Z","shell.execute_reply":"2022-05-13T17:09:32.363308Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"#converting to age by creating new column 'Age Since Remod'\n\nnew_train_set['Age Since Remod'] = new_train_set['YearRemodAdd'].max() - new_train_set['YearRemodAdd']\nnew_test_set['Age Since Remod'] = new_test_set['YearRemodAdd'].max() - new_test_set['YearRemodAdd']\n\n#drop 'YearBuilt'\n\nnew_train_set.drop(['YearRemodAdd'], axis=1, inplace=True)\nnew_test_set.drop(['YearRemodAdd'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:09:32.365635Z","iopub.execute_input":"2022-05-13T17:09:32.366276Z","iopub.status.idle":"2022-05-13T17:09:32.379743Z","shell.execute_reply.started":"2022-05-13T17:09:32.366236Z","shell.execute_reply":"2022-05-13T17:09:32.378956Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"We will now compute the skewness of our features, and then proceed to filter out features which are considered to be too skewed.\n\nWe then apply the log transformation technique to the skewed features.","metadata":{}},{"cell_type":"code","source":"#numerical features\n\nnumerical_features = ['OverallQual', 'GrLivArea', 'GarageArea', 'TotalBsmtSF', 'FullBath', 'Age Since Built', 'Age Since Remod', 'MasVnrArea', 'Fireplaces', 'BsmtFinSF1', 'WoodDeckSF', '2ndFlrSF', 'OpenPorchSF', 'SalePrice']\n\n#applying log transformation to the skewed features for both train and test sets\n\nskewed_features = [x for x in numerical_features if new_train_set[x].skew() >= 0.5]\n\nfor i in skewed_features:\n    if i != 'SalePrice':\n        new_train_set[i] = np.log((new_train_set[i])+1)\n        new_test_set[i] = np.log((new_test_set[i])+1)\n    else:\n        new_train_set[i] = np.log((new_train_set[i])+1)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:09:32.381807Z","iopub.execute_input":"2022-05-13T17:09:32.382152Z","iopub.status.idle":"2022-05-13T17:09:32.408567Z","shell.execute_reply.started":"2022-05-13T17:09:32.382108Z","shell.execute_reply":"2022-05-13T17:09:32.407307Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"#compute the skewness of all numerical features after transformation\n\nprint(f'Feature Skewness after Transformation:\\n')\nprint(new_train_set[numerical_features].skew())\n\n#check the feature distributions after transformation\n\nnew_train_set[numerical_features].hist(bins=50, figsize=(15,15))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:09:32.410532Z","iopub.execute_input":"2022-05-13T17:09:32.410882Z","iopub.status.idle":"2022-05-13T17:09:35.698771Z","shell.execute_reply.started":"2022-05-13T17:09:32.410835Z","shell.execute_reply":"2022-05-13T17:09:35.697684Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"Judging from the computed values of skewness and the distributions above, we can see that most of the features that were previously skewed, are now more normally distributed. ","metadata":{}},{"cell_type":"code","source":"#check the scatter plots (against SalePrice) of our transformed features\n\nreg_plot(new_train_set[numerical_features], numerical_features, 5, 3)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:09:35.699895Z","iopub.execute_input":"2022-05-13T17:09:35.700114Z","iopub.status.idle":"2022-05-13T17:09:40.895550Z","shell.execute_reply.started":"2022-05-13T17:09:35.700087Z","shell.execute_reply":"2022-05-13T17:09:40.894542Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":"From the scatter plots above, we notice previously heteroskedastic variables like 'GrLivArea' and 'TotalBsmtSF' are now more homoskedastic after transformation. As the transformation has improved the quality of our data, they are now ready to be used for modelling.","metadata":{}},{"cell_type":"markdown","source":"**Splitting Data into Training and Testing sets**","metadata":{}},{"cell_type":"code","source":"#define X and y\nX = new_train_set.drop(['SalePrice'], axis=1)\n\ny = new_train_set.loc[:, 'SalePrice']\n\n#splitting the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\n\nprint(f'X_train: {X_train.shape}')\nprint(f'y_train: {y_train.shape}\\n')\nprint(f'X_test: {X_test.shape}')\nprint(f'y_test: {y_test.shape}')","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:09:40.896951Z","iopub.execute_input":"2022-05-13T17:09:40.897289Z","iopub.status.idle":"2022-05-13T17:09:40.910959Z","shell.execute_reply.started":"2022-05-13T17:09:40.897256Z","shell.execute_reply":"2022-05-13T17:09:40.909751Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":"# 4. Predictive Modelling","metadata":{}},{"cell_type":"markdown","source":"# 4.1 Multiple Linear Regression","metadata":{}},{"cell_type":"markdown","source":"We will be using the backward elimination here, in which all features will be fed to the model first. We check the performance of the model and then iteratively remove the worst performing features one by one till the overall performance of the model comes in acceptable range. The performance metric used here to evaluate feature performance is p-value. If the p-value of the feature is more than 0.05, we remove the feature. We also do the same for other features.","metadata":{}},{"cell_type":"code","source":"#Iterative feature selection using Backward Elimination\n\ncols = list(X.columns)\npmax = 1\nwhile (len(cols)>0):\n    p= []\n    X_1 = X[cols]\n    X_1 = smt.add_constant(X_1)\n    model = smt.OLS(y,X_1).fit()\n    p = pd.Series(model.pvalues.values[1:],index = cols)      \n    pmax = max(p)\n    feature_with_p_max = p.idxmax()\n    if(pmax>0.05):\n        cols.remove(feature_with_p_max)\n    else:\n        break\nselected_features_BE = cols\nprint(selected_features_BE)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:09:40.912237Z","iopub.execute_input":"2022-05-13T17:09:40.912539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_LinR = LinearRegression()\nmodel_LinR.fit(X_train[selected_features_BE], y_train)\ny_LinR_pred = model_LinR.predict(X_test[selected_features_BE])\n\nprint( 'R2_Score: ', r2_score(y_test, y_LinR_pred))\nprint(\"RMSE: \", mean_squared_error(y_test, y_LinR_pred, squared=False))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4.2 Ridge Regression","metadata":{}},{"cell_type":"code","source":"alphas = np.linspace(0, 10, 100).tolist()\n\ntuned_parameters = {\"alpha\": alphas}\nridge_cv = GridSearchCV(Ridge(), tuned_parameters, cv=10, n_jobs=-1, verbose=1)\n\nridge_cv.fit(X_train[selected_features_BE], y_train)\n\n# print best params and the corresponding R\nprint(f\"Best hyperparameters: {ridge_cv.best_params_}\")\nprint(f\"Best R (train): {ridge_cv.best_score_}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_ridge_opt = Ridge(alpha = ridge_cv.best_params_[\"alpha\"])\nmodel_ridge_opt.fit(X_train[selected_features_BE], y_train)\ny_pred_ridge_opt = model_ridge_opt.predict(X_test[selected_features_BE])\nacc_ridge_opt = model_ridge_opt.score(X_test[selected_features_BE], y_test)\n\nprint( 'R2_Score: ', acc_ridge_opt)\nprint(\"RMSE: \", mean_squared_error(np.exp(y_test), np.exp(y_pred_ridge_opt), squared=False))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4.3 Lasso Regression","metadata":{}},{"cell_type":"code","source":"alphas = np.linspace(0, 10, 100).tolist()\n\ntuned_parameters = {\"alpha\": alphas}\nlasso_cv = GridSearchCV(Lasso(), tuned_parameters, cv=10, n_jobs=-1, verbose=1)\n\nlasso_cv.fit(X_train[selected_features_BE], y_train)\n\n# print best params and the corresponding R\nprint(f\"Best hyperparameters: {lasso_cv.best_params_}\")\nprint(f\"Best R (train): {lasso_cv.best_score_}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"An optimal value of 0 for its alpha value means the Lasso regression may not be the most ideal model in this case. We shall set the value for alpha to be 0.001.","metadata":{}},{"cell_type":"code","source":"model_lasso = Lasso(alpha = 0.001)\npipe = make_pipeline(StandardScaler(), model_lasso)\npipe.fit(X_train[selected_features_BE], y_train)\ny_pred_ridge = pipe.predict(X_test[selected_features_BE])\nacc_ridge = pipe.score(X_test[selected_features_BE], y_test)\n\nprint( 'R2_Score: ', acc_ridge)\nprint(\"RMSE: \", mean_squared_error(np.exp(y_test), np.exp(y_pred_ridge), squared=False))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4.4 XGBoost Regressor","metadata":{}},{"cell_type":"code","source":"# Define hyperparameters\ntuned_parameters_xgb = {\"max_depth\": [3],\n                        \"colsample_bytree\": [0.3, 0.7],\n                        \"learning_rate\": [0.01, 0.05, 0.1],\n                        \"n_estimators\": [100, 500, 1000]}\n\n# GridSearch\nxgbr_cv = GridSearchCV(estimator=XGBRegressor(),\n                       param_grid=tuned_parameters_xgb,\n                       cv=5,\n                       n_jobs=-1,\n                       verbose=1)\n\n# fit the GridSearch on train set\nxgbr_cv.fit(X_train[selected_features_BE], y_train)\n\n# print best params and the corresponding R\nprint(f\"Best hyperparameters: {xgbr_cv.best_params_}\\n\")\nprint(f\"Best R: {xgbr_cv.best_score_}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_xgb_opt = XGBRegressor(colsample_bytree = xgbr_cv.best_params_[\"colsample_bytree\"],\n                             learning_rate = xgbr_cv.best_params_[\"learning_rate\"],\n                             max_depth = xgbr_cv.best_params_[\"max_depth\"],\n                             n_estimators = xgbr_cv.best_params_[\"n_estimators\"])\n\nmodel_xgb_opt.fit(X_train[selected_features_BE], y_train)\ny_pred_xgb_opt = model_xgb_opt.predict(X_test[selected_features_BE])\nacc_xgb_opt = model_xgb_opt.score(X_test[selected_features_BE], y_test)\n\nprint( 'R2_Score: ', acc_xgb_opt)\nprint(\"RMSE: \", mean_squared_error(np.exp(y_test), np.exp(y_pred_xgb_opt), squared=False))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Prediction Using the Best Model","metadata":{}},{"cell_type":"code","source":"y_pred_with_bestmodel = np.exp(model_LinR.predict(new_test_set[selected_features_BE]))\n\noutput = pd.DataFrame({\"Id\": test_Id_list,\n                       \"SalePrice\": y_pred_with_bestmodel})\n\noutput.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the output\noutput.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}